# -*- coding: utf-8 -*-
"""Keywords.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MY9930jLwC9Tz-2WTNVTDDQdwF3VV3U7
"""



from typing import TypedDict, List
import json
import os

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END

from dotenv import load_dotenv, find_dotenv

_ = load_dotenv(find_dotenv())
api_key = os.getenv("GOOGLE_API_KEY")
llm = ChatGoogleGenerativeAI(
    model="gemini-3-pro-preview",
    temperature=0,
    google_api_key=api_key
)

class SearchState(TypedDict):
    query: str
    raw_keywords: List[str]
    normalized_keywords: List[str]


skill_prompt = ChatPromptTemplate.from_messages([
    ("system",
     "You are a technical skill extraction and expansion engine.\n"
     "Your task:\n"
     "1. Extract all explicit technical skills, tools, programming languages, frameworks, and developer roles.\n"
     "2. Infer closely related technologies that are commonly required together.\n"
     "3. Do NOT add unrelated or speculative skills.\n"
     "4. Normalize synonyms (e.g., 'web dev' â†’ 'web development').\n"
     "5. Prefer industry-standard terminology.\n\n"

     "Expansion rules:\n"

     "- If 'web development' is present, include:\n"
     "  'html', 'css', 'javascript', 'frontend development', 'backend development'.\n"

     "- If 'frontend development' is present, include:\n"
     "  'react', 'ui development'.\n"

     "- If 'backend development' is present, include:\n"
     "  'api development', 'node.js', 'databases'.\n"

     "- If 'machine learning' is present, include:\n"
     "  'python', 'data preprocessing', 'model training', 'model evaluation',\n"
     "  'supervised learning', 'unsupervised learning'.\n"

     "- If 'deep learning' is present, include:\n"
     "  'neural networks', 'tensorflow', 'pytorch'.\n"

     "- If 'data science' is present, include:\n"
     "  'python', 'statistics', 'data analysis', 'pandas', 'numpy'.\n"

     "- If 'cyber security' is present, include:\n"
     "  'network security', 'cryptography', 'vulnerability assessment',\n"
     "  'penetration testing', 'security fundamentals'.\n"

     "- If a programming language is present (e.g., 'python', 'java', 'c++'),\n"
     "  do NOT infer unrelated frameworks or domains.\n"

     "- Do NOT infer cloud platforms, devops, or AI unless explicitly mentioned.\n\n"

     "Output rules:\n"
     "- Return ONLY a valid JSON array.\n"
     "- All values must be lowercase strings.\n"
     "- No explanations, no markdown, no comments.\n"
     "- Output must be pure JSON."
    ),
    ("human", "{query}")
])

def safe_json_parse(text: str):
    """
    Extracts JSON array safely from Gemini output
    """
    match = re.search(r"\[.*\]", text, re.DOTALL)
    if not match:
        raise ValueError(f"No JSON array found in response: {text}")
    return json.loads(match.group())

def extract_skills_node(state: SearchState):
    response = llm.invoke(
        skill_prompt.format_messages(query=state["query"])
    )

    raw_keywords = safe_json_parse(response.content)

    return {
        "raw_keywords": raw_keywords
    }

def normalize_keywords_node(state: SearchState):
    normalized = []

    for kw in state["raw_keywords"]:
        kw_lower = kw.lower()
        normalized.append(SYNONYM_MAP.get(kw_lower, kw_lower))

    return {
        "normalized_keywords": list(set(normalized))
    }

graph = StateGraph(SearchState)

graph.add_node("extract_skills", extract_skills_node)
graph.add_node("normalize_keywords", normalize_keywords_node)

graph.set_entry_point("extract_skills")
graph.add_edge("extract_skills", "normalize_keywords")
graph.add_edge("normalize_keywords", END)

search_graph = graph.compile()

import re

SYNONYM_MAP = {
    "web dev": "web development",
    "ml": "machine learning"
}

def get_normalized_keywords(query: str) -> list[str]:
    """
    Extracts and normalizes technical keywords from a given query string.

    Args:
        query: The input string containing technical skills or roles.

    Returns:
        A list of normalized technical keywords.
    """
    result = search_graph.invoke({
        "query": query
    })
    return result["normalized_keywords"]

# Example usage:
# query_string = "I need someone with specialisation in machine learning, c++, java, web dev"
# final_keywords = get_normalized_keywords(query_string)
# print("Final Keywords:", final_keywords)

# query_str = "I need someone with web development skills"
# answer = get_normalized_keywords(query_str)
# answer

